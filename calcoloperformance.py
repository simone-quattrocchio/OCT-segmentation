# -*- coding: utf-8 -*-
"""CalcoloPerformance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12HhW7zDWLExFpwnVLNBDj5u1zDh4gvpE

**SINCRONIZZAZIONE CON GOOGLE DRIVE**
"""

from google.colab import drive
drive.mount('/content/drive')

"""**IMPORTAZIONE DELLE LIBRERIE NECESSARIE**"""

# Controllo installazione Pytorch
import torch, torchvision
print(torch.__version__, torch.cuda.is_available())

import os
import numpy as np
from zipfile import ZipFile
from PIL import Image
import matplotlib.pyplot as plt
import scipy.io
from scipy import ndimage
from skimage import exposure, morphology,measure
from scipy.ndimage import median_filter,label, generate_binary_structure,binary_erosion,binary_dilation,binary_closing,binary_opening
import skimage.io as io
import cv2
from tqdm import tqdm
from skimage.morphology import disk,remove_small_objects

"""**DEFINIZIONE RETE CONVOLUZIONALE**"""

import torch
import torch.nn as nn

class UNet(nn.Module):
    def __init__(self, input_channels=3, out_classes=1):
        super(UNet, self).__init__()

        # Contracting path
        self.enc1 = self.conv_block(input_channels,32)
        self.enc2 = self.conv_block(32, 64)
        self.enc3 = self.conv_block(64, 128)
        self.enc4 = self.conv_block(128, 256)

        self.pool = nn.MaxPool2d(kernel_size=2)

        # Lowest resolution
        self.bottleneck = self.conv_block(256, 512)
        # Expansive path
        self.upconv4=nn.ConvTranspose2d(512,256, kernel_size=2, stride=2)
        self.dec4 = self.conv_block(512, 256)
        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.dec3 = self.conv_block(384, 128)
        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.dec2 = self.conv_block(256, 64)
        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)
        self.dec1 = self.conv_block(160, 32)

        self.out = nn.Conv2d(32, out_classes, kernel_size=1)

    def conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),

            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )


    def forward(self, x):
        # Contracting path
        x1 = self.enc1(x)

        x2 = self.enc2(self.pool(x1))

        x3 = self.enc3(self.pool(x2))

        x4 = self.enc4(self.pool(x3))

        # Lowest resolution
        x5 = self.bottleneck(self.pool(x4))
        x5=self.upconv4(x5)
        # Expansive path, repeat upconv and concatenation as needed
        x = torch.cat([x4, x5], dim=1)
        x = self.dec4(x)
        x = self.upconv3(x)

        x21=self.upconv3(x4)
        x = torch.cat([x3, x, x21], dim=1)
        x = self.dec3(x)
        x = self.upconv2(x)

        x11=self.upconv2(x3)
        x12=self.upconv2(x21)
        x = torch.cat([x2, x, x11,x12], dim=1)
        x = self.dec2(x)
        x = self.upconv1(x)

        x01=self.upconv1(x2)
        x02=self.upconv1(x11)
        x03=self.upconv1(x12)
        x = torch.cat([x1, x, x01,x02,x03], dim=1)
        x = self.dec1(x)

        x = self.out(x)
        return x

# Esempio di utilizzo
H = 128
W = 128
C = 3
batch_size = 4
X_batch = torch.randn((batch_size, C, H, W))
model=UNet()
output_batch = model(X_batch)

print("Output batch shape:", output_batch.shape)

checkpoint_path='/content/drive/MyDrive/submission/checkpoint_epoch_16.pth'

"""**CARICAMENTO DEL FILE CONFIG**"""

# Caricamento del file config e controllo della correttezza
model = UNet()
model.load_state_dict(torch.load(checkpoint_path)['model_state_dict'])
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

"""**DEFINIZIONE FUNZIONE DI PRE-PROCESSING**"""

# definizione del kernel gaussiano
def kernel_gauss(size, sigma):
  v = np.linspace(-(size-1)/2,(size-1)/2,size)
  x, y = np.meshgrid(v,v)
  h = np.exp(-((x**2 + y**2)/(2.0*sigma**2)))
  h = h/h.sum()
  return h

def pre_processing(image):
  image = cv2.resize(image, (1200,800), interpolation=cv2.INTER_LINEAR)
  image=image.astype(np.float32)/65535 #problema sulla dinamica
  #applicazione del filtro mediano
  image_filt = ndimage.median_filter(image,size=(5,5))
  #applicazione filtro gaussiano
  h = kernel_gauss(3,1)
  I=ndimage.correlate(image_filt,h)
  #equalizzazione adattiva
  image_eq =exposure.equalize_adapthist(I)
  #resize
  image = cv2.resize(image_eq, (512,512), interpolation=cv2.INTER_LINEAR)

  image = np.expand_dims(image, axis=0)
  image = np.expand_dims(image, axis=0)
  image=np.repeat(image, repeats=3, axis=1)

  image = torch.from_numpy(image)

  return image

"""**DEFINIZIONE MODELLO DI INFERENZA**"""

def  inference_model(model,device,image):
  model.eval()
  threshold_value=0.5
  with torch.no_grad():
     image=image.to(device)
     output = model(image)
     predicted_mask = (output.cpu().squeeze() > threshold_value).float()
  return predicted_mask

"""**DEFINIZIONE LISTA CONTENENTE I NOMI DEI VOLUMI DEL DATASET DI INTERESSE**"""

nomi_volumi_train=['w2_d7','w2_d9','w2_d11','w2_d13','d3_p1_w2','w6_d7','w6_d9']
nomi_volumi_val=['w6_d5','w6_d11']

"""**DEFINIZIONE DI FUNZIONI PER IL CALCOLO DEL DICE COEFFICIENT 3D E DELL'ERRORE SUL NUMERO DI ORGANOIDI**

"""

# Coefficiente DICE 3D
def calculate_dsc(mask_manual, mask_auto):
    intersection = np.sum(np.logical_and(mask_manual, mask_auto))
    union = np.sum(np.logical_or(mask_manual, mask_auto))
    dsc = (2.0 * intersection) / ((union + intersection+1e-7))
    return dsc

# Numero di organoidi
def count_organoids_3d(stacked_masks):
  SE=generate_binary_structure(3,3)
  labeled_org, num_org = label(stacked_masks,SE)

  return labeled_org, num_org

# Calcolo errore numerico e percentuale
def num_error_calculation(predicted_volume,real_volume):
  [l1,n1]=count_organoids_3d(predicted_volume)
  [l2,n2]=count_organoids_3d(real_volume)
  err_num=np.abs(n1-n2)
  err_perc=err_num/n2*100
  return err_num,err_perc

"""**DEFINIZIONE ELEMENTO STRUTTURALE PER IL POST-PROCESSING**"""

# Creazione di un elemento strutturale di forma circolare (disco) con raggio 5
disk_radius =5
structuring_element = disk(disk_radius)

"""**CALCOLO DELLE PERFORMANCE**"""

#Inizializzazione delle stringhe contenenti le metriche
dice=np.zeros(7)
num=np.zeros(7)
err_perc=np.zeros(7)

# Caricamento del file .mat contenente le immagini
for k in range(0,len(nomi_volumi_train)):


    mat_file_path = os.path.join('/content/drive/MyDrive/Challenge/DatasetOCT_unzipped/DatasetOCT/volumes',nomi_volumi_train[k])+'.mat'
    mat_data = scipy.io.loadmat(mat_file_path)
    # Accesso alle immagini dal file caricato
    images = mat_data['V']
    [A,B,C]=images.shape
    predicted_volume= np.zeros([C,A,B])
    real_volume=np.zeros([C,A,B])

    # Loop sulle immagini
    for i in tqdm(range(0,698)):
          image = images[:,:,i]
          ###############################
          ####### Pre-Processing ########
          ###############################

          image_preproc = pre_processing(image)

          ###############################
          #######    Inferenza   ########
          ###############################

          pred_label = inference_model(model,device,image_preproc)
          pred_label_np = pred_label.cpu().numpy()
          pred_label_np= cv2.resize(pred_label_np, (1200,A), interpolation=cv2.INTER_LINEAR)

          ###############################
          ####### Post-Processing #######
          ###############################

          opened_mask = binary_opening(pred_label_np,structuring_element)

          ###############################
          #######   Save results  #######
          ###############################

          #stack delle predizioni
          predicted_volume[i,:,:] = ((opened_mask * 255).astype(np.uint8))

    # Creazione volume contenente le maschere manuali
    for m in range(1,len(os.listdir(os.path.join('/content/drive/MyDrive/Challenge/DatasetOCT_unzipped/DatasetOCT/masks',nomi_volumi_train[k])))):
      filename='/'+nomi_volumi_train[k]+'_' + str(m) + '.png'
      imm=np.array(Image.open('/content/drive/MyDrive/Challenge/DatasetOCT_unzipped/DatasetOCT/masks/'+nomi_volumi_train[k]+filename))
      real_volume[m,:,:] = imm


    #calcolo delle metriche
    dice[k]=calculate_dsc(real_volume,predicted_volume)

    [num[k],err_perc[k]]= num_error_calculation(predicted_volume,real_volume)

# calcolo di media e deviazione standard
dice_medio=np.mean(dice)
dice_std=np.std(dice)
num_medio=np.mean(num)
num_std=np.std(num)
err_perc_medio=np.mean(err_perc)
err_perc_std=np.std(err_perc)

print(f'Dice 3D:{dice}')
print(f'Dice 3D medio:{dice_medio}')
print(f'Deviazione standard:{dice_std}')

print(f'Errore numerico conteggio organoidi:{num}')
print(f'Errore numerico medio conteggio organoidi:{num_medio}')
print(f'Deviazione standard:{num_std}')

print(f'Errore percentuale conteggio organoidi:{err_perc}')
print(f'Errore percentuale medio conteggio organoidi:{err_perc_medio}')
print(f'Deviazione standard:{err_perc_std}')