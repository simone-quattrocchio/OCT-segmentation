# -*- coding: utf-8 -*-
"""Datasplit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12HTz437MV3ZoV7sgmeMb3ST1Ed5HUozD

**SINCRONIZZAZIONE CON GOOGLE DRIVE**
"""

from google.colab import drive
drive.mount('/content/drive')

"""**IMPORTAZIONE DELLE LIBRERIE NECESSARIE**"""

import os
import numpy as np
from zipfile import ZipFile
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm
import scipy.io
from scipy import ndimage
from skimage import exposure, filters, morphology
from scipy.ndimage import gaussian_filter, median_filter
from skimage.morphology import square
import skimage.io as io
import cv2
import shutil
from skimage.restoration import denoise_nl_means

"""**INIZIALIZZAZIONE FOLDERS DATASET IN TRAINING,VALIDATION E TEST**"""

# Percorso principale per il dataset elaborato
path_proc = '/content/drive/MyDrive/Challenge/DatasetProc'

# Verifica e creazione delle cartelle necessarie se non esistono
if not os.path.exists(path_proc):
    os.makedirs(path_proc)

# Creazione della cartella 'volumes' se non esiste
if not os.path.exists(os.path.join(path_proc, 'volumes')):
    os.makedirs(os.path.join(path_proc, 'volumes'))

# Creazione della cartella 'masks' se non esiste
if not os.path.exists(os.path.join(path_proc, 'masks')):
    os.makedirs(os.path.join(path_proc, 'masks'))

# Creazione della cartella 'train' dentro 'volumes' se non esiste
if not os.path.exists(os.path.join(path_proc, 'volumes', 'train')):
    os.makedirs(os.path.join(path_proc, 'volumes', 'train'))

# Creazione della cartella 'val' dentro 'volumes' se non esiste
if not os.path.exists(os.path.join(path_proc, 'volumes', 'val')):
    os.makedirs(os.path.join(path_proc, 'volumes', 'val'))

# Creazione della cartella 'test' dentro 'volumes' se non esiste
if not os.path.exists(os.path.join(path_proc, 'volumes', 'test')):
    os.makedirs(os.path.join(path_proc, 'volumes', 'test'))

# Creazione della cartella 'train' dentro 'masks' se non esiste
if not os.path.exists(os.path.join(path_proc, 'masks', 'train')):
    os.makedirs(os.path.join(path_proc, 'masks', 'train'))

# Creazione della cartella 'val' dentro 'masks' se non esiste
if not os.path.exists(os.path.join(path_proc, 'masks', 'val')):
    os.makedirs(os.path.join(path_proc, 'masks', 'val'))

# Creazione della cartella 'test' dentro 'masks' se non esiste
if not os.path.exists(os.path.join(path_proc, 'masks', 'test')):
    os.makedirs(os.path.join(path_proc, 'masks', 'test'))

"""**DEFINIZIONE DEI PREPROCESSING**

In questi blocchi si definiscono i preprocessing da applicare prima del salvataggio delle immagini.
"""

#Funzione per generare un kernel gaussiano 2D
def kernel_gauss(size, sigma):
  v = np.linspace(-(size-1)/2,(size-1)/2,size)
  x, y = np.meshgrid(v,v)
  h = np.exp(-((x**2 + y**2)/(2.0*sigma**2)))
  h = h/h.sum()
  return h

import cv2
import numpy as np
from scipy import ndimage
from skimage import exposure
from skimage.restoration import denoise_nl_means

# Definizione della funzione di training 1
def training1(image):
    # Ridimensionamento dell'immagine a 1200x800 pixel
    image = cv2.resize(image, (1200, 800), interpolation=cv2.INTER_LINEAR)
    # Normalizzazione dei valori dei pixel nell'intervallo [0, 1]
    image = image / 65535
    # Applicazione di un filtro mediano con kernel 5x5
    image_filt = ndimage.median_filter(image, size=(5, 5))
    # Generazione del kernel gaussiano con dimensione 3 e sigma 1
    h = kernel_gauss(3, 1)
    # Applicazione del filtro gaussiano all'immagine
    I = ndimage.correlate(image_filt, h)

    return I

# Definizione della funzione di training 2
def training2(image):
    image = cv2.resize(image, (1200, 800), interpolation=cv2.INTER_LINEAR)
    image = image / 65535
    image_filt = ndimage.median_filter(image, size=(5, 5))
    h = kernel_gauss(3, 1)
    I = ndimage.correlate(image_filt, h)  # Applicazione del filtro gaussiano
    image_eq = exposure.equalize_adapthist(I)  # Equalizzazione istogramma adattiva

    return image_eq

# Definizione della funzione di training 3
def training3(image):
    image = cv2.resize(image, (1200, 800), interpolation=cv2.INTER_LINEAR)
    image = image / 65535
    image_filt = ndimage.median_filter(image, size=(5, 5))
    h = kernel_gauss(3, 1)
    I = ndimage.correlate(image_filt, h)  # Applicazione del filtro gaussiano
    image_eq = exposure.equalize_adapthist(I)  # Equalizzazione istogramma adattiva

    # Definizione dei filtri diagonali
    h_45 = np.array([[2, -1, -1], [-1, 2, -1], [-1, -1, 2]])
    h_45_2 = np.array([[-1, -1, 2], [-1, 2, -1], [2, -1, -1]])
    # Applicazione dei filtri diagonali
    g1 = ndimage.correlate(image_eq, h_45)
    g2 = ndimage.correlate(image_eq, h_45_2)
    # Combinazione delle immagini filtrate
    im_filt = image_eq + np.abs(g1) + np.abs(g2)

    return im_filt

# Definizione della funzione di training 4
def training4(image):
    image = cv2.resize(image, (1200, 800), interpolation=cv2.INTER_LINEAR)
    image = image / 65535
    image = denoise_nl_means(image, patch_size=9, patch_distance=5, h=0.1)  # Denoising non locale
    image = exposure.equalize_adapthist(image)  # Equalizzazione istogramma adattiva

    return image

# Impostazione della funzione di preprocessing da utilizzare
area_segm = training1  # Sostituire con training2, training3, o training4 per scegliere altre opzioni di preprocessing

"""## Creazione del test set

"""

base_path = '/content/drive/MyDrive/Challenge/DatasetOCT/'

# Iterazione sui file nella cartella 'volumes'
for volume_file in os.listdir(os.path.join(base_path, 'volumes')):
    head, tail = os.path.splitext(volume_file)

    # Verifica se il file è un file .mat con nome 'w2_d5' o 'w6_d13'
    if tail == '.mat' and (head == 'w2_d5' or head == 'w6_d13'):
        volume_file_path = os.path.join(base_path, 'volumes', volume_file)

        # Carica il volume dall'archivio .mat
        volume = scipy.io.loadmat(volume_file_path)['V']

        # Iterazione su 698 immagini nel volume
        for i in np.arange(0, 698):
            # Applica la funzione di segmentazione (area_segm) al volume slice-by-slice
            img_proc = area_segm(volume[:, :, i])

            # Specifica la destinazione per salvare le immagini elaborate
            destination = '/content/drive/MyDrive/Challenge/DatasetProc/volumes/test'

            # Salva l'immagine processata come file .png usando plt.imsave
            plt.imsave(os.path.join(destination, head + f'_{i + 1}.png'), img_proc, cmap='gray')

        # Iterazione su 698 maschere associate al volume
        for i in np.arange(1, 699):
            # Carica la maschera dal percorso specificato
            mask_path = os.path.join(base_path, 'masks', head, f'{head}_{i}.png')
            mask = Image.open(mask_path)
            mask = np.array(mask)

            # Ridimensiona la maschera a 1200x800 pixel
            mask_resized = cv2.resize(mask, (1200, 800), interpolation=cv2.INTER_LINEAR)

            # Specifica la destinazione per salvare le maschere ridimensionate
            destination = '/content/drive/MyDrive/Challenge/DatasetProc/masks/test'

            # Salva la maschera ridimensionata come file .png
            Image.fromarray(mask_resized).save(os.path.join(destination, f'{head}_{i}.png'))

# Verifica se le liste di file nella cartella delle maschere e dei volumi sono identiche
masks = os.listdir('/content/drive/MyDrive/Challenge/DatasetProc/masks/test')
volumes = os.listdir('/content/drive/MyDrive/Challenge/DatasetProc/volumes/test')

# Confronto tra le liste di file per verificare se sono identiche
files_match = masks == volumes

"""## Creazione del validation set"""

# Iterazione sui file nella cartella 'volumes'
for volume_file in os.listdir(os.path.join(base_path, 'volumes')):
    head, tail = os.path.splitext(volume_file)

    # Verifica se il file è un file .mat con nome 'w6_d11' o 'w6_d5'
    if tail == '.mat' and (head == 'w6_d11' or head == 'w6_d5'):
        volume_file_path = os.path.join(base_path, 'volumes', volume_file)

        # Carica il volume dall'archivio .mat
        volume = scipy.io.loadmat(volume_file_path)['V']

        # Iterazione su 698 immagini nel volume
        for i in np.arange(0, 698):
            # Estrae la slice corrente dal volume
            slice_volume = volume[:, :, i]

            # Applica la funzione di segmentazione (area_segm) alla slice
            img_proc = area_segm(slice_volume)

            # Specifica la destinazione per salvare le immagini processate nella cartella 'val'
            destination = '/content/drive/MyDrive/Challenge/DatasetProc/volumes/val'

            # Salva l'immagine processata come file .png usando plt.imsave
            plt.imsave(os.path.join(destination, f'{head}_{i + 1}.png'), img_proc, cmap='gray')

        # Iterazione su 698 maschere associate al volume
        for i in np.arange(1, 699):
            # Costruisce il percorso per caricare la maschera corrispondente
            mask_path = os.path.join(base_path, 'masks', head, f'{head}_{i}.png')

            # Carica la maschera come un array numpy
            mask = np.array(Image.open(mask_path))

            # Ridimensiona la maschera a 1200x800 pixel usando OpenCV
            mask_resized = cv2.resize(mask, (1200, 800), interpolation=cv2.INTER_LINEAR)

            # Specifica la destinazione per salvare le maschere ridimensionate nella cartella 'val'
            destination = '/content/drive/MyDrive/Challenge/DatasetProc/masks/val'

            # Salva la maschera ridimensionata come file .png
            Image.fromarray(mask_resized).save(os.path.join(destination, f'{head}_{i}.png'))

# Verifica se le liste di file nella cartella delle maschere e dei volumi sono identiche
masks = os.listdir('/content/drive/MyDrive/Challenge/DatasetProc/masks/val')
volumes = os.listdir('/content/drive/MyDrive/Challenge/DatasetProc/volumes/val')
files_match = masks == volumes  # True se le liste sono identiche, altrimenti False

"""## Creazione del training set"""

# Iterazione sui file nella cartella 'volumes'
for volume_file in os.listdir(os.path.join(base_path, 'volumes')):
    head, tail = os.path.splitext(volume_file)

    # Escludere i volumi che appartengono ai set 'val' e 'test'
    if tail == '.mat' and head != 'w6_d13' and head != 'w2_d5' and head != 'w6_d11' and head != 'w6_d5':
        volume_file_path = os.path.join(base_path, 'volumes', volume_file)

        # Carica il volume dall'archivio .mat
        volume = scipy.io.loadmat(volume_file_path)['V']

        # Iterazione su 698 immagini nel volume
        for i in np.arange(0, 698):
            # Estrae la slice corrente dal volume
            slice_volume = volume[:, :, i]

            # Applica la funzione di segmentazione (area_segm) alla slice
            img_proc = area_segm(slice_volume)

            # Specifica la destinazione per salvare le immagini processate nella cartella 'train'
            destination = '/content/drive/MyDrive/Challenge/DatasetProc/volumes/train'

            # Salva l'immagine processata come file .png usando plt.imsave
            plt.imsave(os.path.join(destination, f'{head}_{i + 1}.png'), img_proc, cmap='gray')

        # Iterazione su 698 maschere associate al volume
        for i in np.arange(1, 699):
            # Costruisce il percorso per caricare la maschera corrispondente
            mask_path = os.path.join(base_path, 'masks', head, f'{head}_{i}.png')

            # Carica la maschera come un array numpy
            mask = np.array(Image.open(mask_path))

            # Ridimensiona la maschera a 1200x800 pixel usando OpenCV
            mask_resized = cv2.resize(mask, (1200, 800), interpolation=cv2.INTER_LINEAR)

            # Specifica la destinazione per salvare le maschere ridimensionate nella cartella 'train'
            destination = '/content/drive/MyDrive/Challenge/DatasetProc/masks/train'

            # Salva la maschera ridimensionata come file .png
            Image.fromarray(mask_resized).save(os.path.join(destination, f'{head}_{i}.png'))

# Verifica se le liste di file nella cartella delle maschere e dei volumi sono identiche
masks = os.listdir('/content/drive/MyDrive/Challenge/DatasetProc/masks/train')
volumes = os.listdir('/content/drive/MyDrive/Challenge/DatasetProc/volumes/train')
files_match = masks == volumes  # True se le liste sono identiche, altrimenti False
